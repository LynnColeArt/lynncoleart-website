---
layout: post.njk
title: How to Be Untrainable
date: '2023-12-31T03:07:00.000Z'
tags:
  - ai art
  - training
  - ethical ai
excerpt: >-
  I've been debating whether to comment on this conversation lately. The aducity
  of it, to be as boldly wrong as it is.So what's happened recently, is that a
  number of small but loud people in the art community have tried, and succeeded
  in convincing small obscure artists with no presence outside of twitter and
  artstation, that AI can replicate their work. You know this isn't true, I know
  this isn't true, but why it's untrue comes from the way generalization works,
  generally. So I thought I would cover the topic in yet another... how to be
  untrainable discussion.
draft: false
---
I've been debating whether to comment on this conversation lately. The aducity of it, to be as boldly wrong as it is. 

So what's happened recently, is that a number of small but loud people in the art community have tried, and succeeded in convincing small obscure artists with no presence outside of twitter and artstation, that AI can replicate their work. You know this isn't true, I know this isn't true, but why it's untrue comes from the way generalization works, generally. So I thought I would cover the topic in yet another... how to be untrainable discussion.

Fun fun.

Let's do this thing.

## Misconception #1: AI can perfectly copy small artists

Not true. Most individual artists barely show up in the training set—fifteen, maybe twenty images indexed tops. That’s nowhere near enough to reliably reproduce a style at the foundation model level.

Punch in a random artist? You’ll mostly get “default style”: a photo-referenced blend of contemporary art clichés. Even big names with huge cultural presence (Android Jones, Alex Ross) admitted early AI tools weren’t successfully cloning them.

---

## Misconception #2: AI can copy *anything*

Again—no. The system compresses data into attributes, averages, and probabilities. The flatter the data, the flatter the result. Simple prompts in, simple outputs out.

The only real magic happens when you break the machine: overload it with symmetry, noise, or ideas it can’t quite reconcile. That’s when you get the weird, the trippy, the untrainable.

---

## How to Be Untrainable

So, if you actually want to make work that resists machine mimicry, here are a dozen ways to gum up the gears:

1. Develop a unique, idiosyncratic style.
2. Fine lines, everywhere.
3. Elaborate symmetry.
4. Non-sequitur abstraction.
5. Dirt faces (teasing faces where they don’t belong).
6. Chaotic, unpredictable color noise.
7. Unconventional textures.
8. Mix 2D and 3D in the same frame.
9. Dirt people (false figures in the margins).
10. Hide logographic symbols in plain sight.
11. Lean into common AI artifacts—subtly cue the glitches.
12. Or the golden rule: *WWJKD?* (What Would Jack Kirby Do?)

---

## The Punchline

The dirty secret is AI doesn’t steal as much as people think. It stumbles, it averages, it flattens. The really interesting part is: you can *make yourself untrainable*.

That’s the move.
